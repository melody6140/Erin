#!/usr/bin/env python3
"""
æ”¹è¿›çš„ASRMæ¨¡å— (Attention-guided Semantic Recalibration Module)
è§£å†³åŸå§‹å®ç°ä¸­çš„è¿‡åº¦æ­£åˆ™åŒ–å’Œè¯­ä¹‰ç ´åé—®é¢˜
"""

import torch
import torch.nn as nn
import torch.nn.functional as F


class ImprovedASRM(nn.Module):
    """
    æ”¹è¿›çš„æ³¨æ„åŠ›å¼•å¯¼è¯­ä¹‰æ ¡å‡†æ¨¡å—

    ä¸»è¦æ”¹è¿›ï¼š
    1. æ·»åŠ æ®‹å·®è¿æ¥ï¼Œä¿æŒåŸå§‹è¯­ä¹‰ä¿¡æ¯
    2. ç§»é™¤è¿‡åº¦æ­£åˆ™åŒ–çš„dropout
    3. ä½¿ç”¨æ›´ç»†ç²’åº¦çš„è¯­ä¹‰æ•è·
    4. æ·»åŠ å¯å­¦ä¹ çš„èåˆæƒé‡
    """

    def __init__(self, hidden_size: int = 768, reduction_ratio: int = 8,
                 use_max_pooling: bool = False, residual_weight: float = 0.5):
        super(ImprovedASRM, self).__init__()

        self.hidden_size = hidden_size
        self.reduction_ratio = reduction_ratio
        self.use_max_pooling = use_max_pooling

        # ä½¿ç”¨æ›´å°çš„å‹ç¼©æ¯”ä¾‹ï¼Œä¿ç•™æ›´å¤šä¿¡æ¯
        reduced_size = max(1, hidden_size // reduction_ratio)

        # æ”¹è¿›çš„æ¿€åŠ±ç½‘ç»œ
        self.excitation = nn.Sequential(
            nn.Linear(hidden_size, reduced_size),
            nn.SiLU(),  # ä½¿ç”¨SiLUæ¿€æ´»å‡½æ•°ï¼Œæ¯”ReLUæ›´å¹³æ»‘
            nn.Linear(reduced_size, hidden_size),
            nn.Sigmoid()
        )

        # å¯å­¦ä¹ çš„æ®‹å·®è¿æ¥æƒé‡
        self.residual_weight = nn.Parameter(torch.tensor(residual_weight))

        # å±‚å½’ä¸€åŒ–ï¼Œç¨³å®šè®­ç»ƒ
        self.layer_norm = nn.LayerNorm(hidden_size)

        # ç§»é™¤dropoutï¼Œå‡å°‘è¿‡åº¦æ­£åˆ™åŒ–

    def forward(self, hidden_states: torch.Tensor,
                apply_augmentation: bool = True) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­

        Args:
            hidden_states: [batch_size, seq_len, hidden_size]
            apply_augmentation: æ˜¯å¦åº”ç”¨å¢å¼º

        Returns:
            enhanced_states: å¢å¼ºåçš„éšè—çŠ¶æ€
        """
        batch_size, seq_len, hidden_size = hidden_states.shape

        # 1. å…¨å±€ä¿¡æ¯æå–ï¼ˆæ”¹è¿›ç‰ˆï¼‰
        if self.use_max_pooling:
            # ä½¿ç”¨æœ€å¤§æ± åŒ–æ•è·æ˜¾è‘—ç‰¹å¾
            global_info = torch.max(hidden_states, dim=1)[0]
        else:
            # ä½¿ç”¨å¹³å‡æ± åŒ–æ•è·æ•´ä½“è¯­ä¹‰
            global_info = torch.mean(hidden_states, dim=1)

        # 2. ç”Ÿæˆæ³¨æ„åŠ›æƒé‡
        attention_weights = self.excitation(global_info)  # [batch_size, hidden_size]
        attention_weights = attention_weights.unsqueeze(1)  # [batch_size, 1, hidden_size]

        # 3. æ ¡å‡†æ“ä½œ
        calibrated_states = attention_weights * hidden_states

        # 4. æ®‹å·®è¿æ¥ï¼ˆå…³é”®æ”¹è¿›ï¼‰
        if apply_augmentation:
            # ä½¿ç”¨å¯å­¦ä¹ çš„æƒé‡èåˆåŸå§‹å’Œæ ¡å‡†åçš„ç‰¹å¾
            enhanced_states = (
                torch.sigmoid(self.residual_weight) * calibrated_states +
                (1 - torch.sigmoid(self.residual_weight)) * hidden_states
            )
        else:
            enhanced_states = calibrated_states

        # 5. å±‚å½’ä¸€åŒ–
        enhanced_states = self.layer_norm(enhanced_states)

        return enhanced_states


class AdaptiveASRM(nn.Module):
    """
    è‡ªé€‚åº”ASRMæ¨¡å—
    æ ¹æ®è¾“å…¥åŠ¨æ€è°ƒæ•´æ ¡å‡†å¼ºåº¦
    """

    def __init__(self, hidden_size: int = 768, reduction_ratio: int = 8):
        super(AdaptiveASRM, self).__init__()

        self.hidden_size = hidden_size

        # æ ¡å‡†å¼ºåº¦é¢„æµ‹å™¨
        self.intensity_predictor = nn.Sequential(
            nn.Linear(hidden_size, hidden_size // 4),
            nn.ReLU(),
            nn.Linear(hidden_size // 4, 1),
            nn.Sigmoid()
        )

        # åŸºç¡€ASRMæ¨¡å—
        self.base_asrm = ImprovedASRM(hidden_size, reduction_ratio)

    def forward(self, hidden_states: torch.Tensor,
                apply_augmentation: bool = True) -> torch.Tensor:
        """
        è‡ªé€‚åº”å‰å‘ä¼ æ’­
        """
        # é¢„æµ‹æ ¡å‡†å¼ºåº¦
        global_context = torch.mean(hidden_states, dim=1)  # [batch_size, hidden_size]
        intensity = self.intensity_predictor(global_context)  # [batch_size, 1]

        # åº”ç”¨åŸºç¡€ASRM
        enhanced_states = self.base_asrm(hidden_states, apply_augmentation)

        # æ ¹æ®é¢„æµ‹å¼ºåº¦è°ƒæ•´è¾“å‡º
        intensity = intensity.unsqueeze(1)  # [batch_size, 1, 1]
        adaptive_output = (
            intensity * enhanced_states +
            (1 - intensity) * hidden_states
        )

        return adaptive_output


class MultiScaleASRM(nn.Module):
    """
    å¤šå°ºåº¦ASRMæ¨¡å—
    åœ¨ä¸åŒå°ºåº¦ä¸Šè¿›è¡Œè¯­ä¹‰æ ¡å‡†
    """

    def __init__(self, hidden_size: int = 768, num_scales: int = 3):
        super(MultiScaleASRM, self).__init__()

        self.hidden_size = hidden_size
        self.num_scales = num_scales

        # å¤šå°ºåº¦ASRMæ¨¡å—
        self.scale_modules = nn.ModuleList([
            ImprovedASRM(hidden_size, reduction_ratio=4 * (i + 1))
            for i in range(num_scales)
        ])

        # å°ºåº¦èåˆæƒé‡
        self.scale_weights = nn.Parameter(torch.ones(num_scales) / num_scales)

        # æœ€ç»ˆèåˆå±‚
        self.fusion = nn.Linear(hidden_size * num_scales, hidden_size)

    def forward(self, hidden_states: torch.Tensor,
                apply_augmentation: bool = True) -> torch.Tensor:
        """
        å¤šå°ºåº¦å‰å‘ä¼ æ’­
        """
        scale_outputs = []

        # åœ¨ä¸åŒå°ºåº¦ä¸Šåº”ç”¨ASRM
        for i, scale_module in enumerate(self.scale_modules):
            scale_output = scale_module(hidden_states, apply_augmentation)
            scale_outputs.append(scale_output)

        # åŠ æƒèåˆ
        weighted_outputs = []
        for i, output in enumerate(scale_outputs):
            weight = F.softmax(self.scale_weights, dim=0)[i]
            weighted_outputs.append(weight * output)

        # æ‹¼æ¥å¹¶èåˆ
        concatenated = torch.cat(scale_outputs, dim=-1)
        fused_output = self.fusion(concatenated)

        return fused_output


def test_improved_asrm():
    """æµ‹è¯•æ”¹è¿›çš„ASRMæ¨¡å—"""
    print("ğŸ§ª æµ‹è¯•æ”¹è¿›çš„ASRMæ¨¡å—...")

    # åˆ›å»ºæµ‹è¯•æ•°æ®
    batch_size, seq_len, hidden_size = 2, 10, 768
    test_input = torch.randn(batch_size, seq_len, hidden_size)

    # æµ‹è¯•åŸºç¡€æ”¹è¿›ASRM
    print("\n1ï¸âƒ£ æµ‹è¯•åŸºç¡€æ”¹è¿›ASRM...")
    improved_asrm = ImprovedASRM(hidden_size=hidden_size)
    output1 = improved_asrm(test_input, apply_augmentation=True)
    print(f"âœ… è¾“å…¥å½¢çŠ¶: {test_input.shape} -> è¾“å‡ºå½¢çŠ¶: {output1.shape}")

    # æµ‹è¯•è‡ªé€‚åº”ASRM
    print("\n2ï¸âƒ£ æµ‹è¯•è‡ªé€‚åº”ASRM...")
    adaptive_asrm = AdaptiveASRM(hidden_size=hidden_size)
    output2 = adaptive_asrm(test_input, apply_augmentation=True)
    print(f"âœ… è¾“å…¥å½¢çŠ¶: {test_input.shape} -> è¾“å‡ºå½¢çŠ¶: {output2.shape}")

    # æµ‹è¯•å¤šå°ºåº¦ASRM
    print("\n3ï¸âƒ£ æµ‹è¯•å¤šå°ºåº¦ASRM...")
    multiscale_asrm = MultiScaleASRM(hidden_size=hidden_size)
    output3 = multiscale_asrm(test_input, apply_augmentation=True)
    print(f"âœ… è¾“å…¥å½¢çŠ¶: {test_input.shape} -> è¾“å‡ºå½¢çŠ¶: {output3.shape}")

    print("\nğŸ‰ æ‰€æœ‰ASRMæ¨¡å—æµ‹è¯•é€šè¿‡ï¼")


if __name__ == "__main__":
    test_improved_asrm()

