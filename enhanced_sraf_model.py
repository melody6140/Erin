#!/usr/bin/env python3
"""
å¢å¼ºçš„SRAFæ¨¡å‹
é›†æˆSpanAware ASRMï¼Œè§£å†³åŸå§‹ASRMçš„è¯­ä¹‰å±‚æ¬¡ä¸åŒ¹é…é—®é¢˜
"""

from typing import Dict, Optional

import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import AutoModel, AutoConfig

# å¯¼å…¥SpanAware ASRM
from span_aware_asrm import SpanAwareASRM, ContrastiveFriendlyASRM, HybridDCLASRM


class AttentionEntropyRegularizer(nn.Module):
    """æ³¨æ„åŠ›ç†µæ­£åˆ™åŒ–å™¨"""

    def __init__(self):
        super(AttentionEntropyRegularizer, self).__init__()

    def compute_attention_entropy(self, attention_weights: torch.Tensor) -> torch.Tensor:
        """
        è®¡ç®—æ³¨æ„åŠ›ç†µ
        Args:
            attention_weights: [batch_size, num_heads, seq_len, seq_len]
        Returns:
            æ³¨æ„åŠ›ç†µ [scalar]
        """
        eps = 1e-8
        attention_weights_safe = attention_weights.clone() + eps

        # è®¡ç®—ç†µ: H(Î±) = -Î£(Î±_i * log(Î±_i))
        entropy = -torch.sum(attention_weights_safe * torch.log(attention_weights_safe), dim=-1)

        # å¯¹æ‰€æœ‰å¤´å’Œä½ç½®æ±‚å¹³å‡
        mean_entropy = torch.mean(entropy)

        return mean_entropy


class CognitiveDistortionActivationOperator(nn.Module):
    """è®¤çŸ¥æ‰­æ›²æ¿€æ´»ç®—å­ (CDAO)"""

    def __init__(self, hidden_size: int, num_distortion_types: int = 5):
        super(CognitiveDistortionActivationOperator, self).__init__()
        self.classifier = nn.Sequential(
            nn.Linear(hidden_size, hidden_size // 2),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_size // 2, num_distortion_types)
        )

    def forward(self, pooled_output: torch.Tensor) -> torch.Tensor:
        """
        Args:
            pooled_output: [batch_size, hidden_size]
        Returns:
            è®¤çŸ¥æ‰­æ›²é¢„æµ‹ [batch_size, num_distortion_types]
        """
        return self.classifier(pooled_output)


class ContrastiveLearningHead(nn.Module):
    """å¯¹æ¯”å­¦ä¹ å¤´"""

    def __init__(self, hidden_size: int, projection_dim: int = 128):
        super(ContrastiveLearningHead, self).__init__()
        self.projection = nn.Sequential(
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, projection_dim)
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Args:
            x: [batch_size, hidden_size]
        Returns:
            æŠ•å½±ç‰¹å¾ [batch_size, projection_dim]
        """
        return F.normalize(self.projection(x), p=2, dim=1)


class EnhancedSRAFModel(nn.Module):
    """
    å¢å¼ºçš„SRAFæ¨¡å‹
    é›†æˆSpanAware ASRMï¼Œè§£å†³åŸå§‹å®ç°çš„é—®é¢˜
    """

    def __init__(self, config, asrm_type: str = 'span_aware'):
        super(EnhancedSRAFModel, self).__init__()
        self.config = config

        # åŠ è½½é¢„è®­ç»ƒBERTæ¨¡å‹
        self.bert_config = AutoConfig.from_pretrained(config.model_name)
        self.bert_config.output_attentions = True
        self.bert = AutoModel.from_pretrained(config.model_name, config=self.bert_config)

        # é€‰æ‹©ASRMç±»å‹
        if asrm_type == 'span_aware':
            self.asrm = SpanAwareASRM(
                hidden_size=config.hidden_size,
                window_sizes=[1, 3, 5],
                reduction_ratio=4
            )
        elif asrm_type == 'contrastive_friendly':
            self.asrm = ContrastiveFriendlyASRM(
                hidden_size=config.hidden_size,
                temperature=config.contrastive_temperature
            )
        elif asrm_type == 'hybrid_dcl':
            self.asrm = HybridDCLASRM(
                hidden_size=config.hidden_size,
                num_classes=config.num_labels
            )
        else:
            raise ValueError(f"Unknown ASRM type: {asrm_type}")

        self.asrm_type = asrm_type

        # æ³¨æ„åŠ›ç†µæ­£åˆ™åŒ–å™¨
        self.entropy_regularizer = AttentionEntropyRegularizer()

        # è®¤çŸ¥æ‰­æ›²æ¿€æ´»ç®—å­
        self.cdao = CognitiveDistortionActivationOperator(
            hidden_size=config.hidden_size,
            num_distortion_types=config.num_cognitive_distortions
        )

        # å¯¹æ¯”å­¦ä¹ å¤´
        self.contrastive_head = ContrastiveLearningHead(config.hidden_size)

        # ä¸»åˆ†ç±»å™¨
        self.classifier = nn.Sequential(
            nn.Dropout(config.dropout),
            nn.Linear(config.hidden_size, config.num_labels)
        )

    def forward(
        self,
        input_ids: torch.Tensor,
        attention_mask: torch.Tensor,
        labels: Optional[torch.Tensor] = None,
        cognitive_distortion_labels: Optional[torch.Tensor] = None,
        return_contrastive_features: bool = False
    ) -> Dict[str, torch.Tensor]:
        """
        å‰å‘ä¼ æ’­
        """
        # BERTç¼–ç 
        outputs = self.bert(
            input_ids=input_ids,
            attention_mask=attention_mask,
            return_dict=True
        )

        sequence_output = outputs.last_hidden_state
        pooled_output = outputs.pooler_output
        attentions = outputs.attentions

        # åº”ç”¨å¢å¼ºçš„ASRM
        if self.asrm_type == 'contrastive_friendly':
            asrm_outputs = self.asrm(
                sequence_output,
                attention_mask,
                labels,
                return_contrastive_features=True
            )
            enhanced_pooled_output = asrm_outputs['cls_features']
            logits = asrm_outputs['classification_logits']
            contrastive_features = asrm_outputs.get('contrastive_features', None)
        elif self.asrm_type == 'hybrid_dcl':
            asrm_outputs = self.asrm(sequence_output, attention_mask, labels)
            enhanced_pooled_output = asrm_outputs['cls_features']
            logits = asrm_outputs['classification_logits']
            contrastive_features = asrm_outputs['supervised_features']
        else:  # span_aware
            enhanced_sequence_output = self.asrm(
                sequence_output, attention_mask, labels, apply_augmentation=True
            )
            enhanced_pooled_output = torch.mean(enhanced_sequence_output, dim=1)
            logits = self.classifier(enhanced_pooled_output)
            contrastive_features = self.contrastive_head(enhanced_pooled_output)

        # è®¤çŸ¥æ‰­æ›²é¢„æµ‹
        cdao_logits = self.cdao(enhanced_pooled_output)

        # è®¡ç®—æ³¨æ„åŠ›ç†µ
        attention_entropy = 0
        if attentions:
            for attention in attentions:
                attention_entropy += self.entropy_regularizer.compute_attention_entropy(attention)
            attention_entropy /= len(attentions)

        results = {
            'logits': logits,
            'cdao_logits': cdao_logits,
            'contrastive_features': contrastive_features,
            'attention_entropy': attention_entropy,
            'enhanced_features': enhanced_pooled_output
        }

        # è®¡ç®—æŸå¤±
        if labels is not None:
            loss_dict = self.compute_loss(
                logits=logits,
                labels=labels,
                cdao_logits=cdao_logits,
                cognitive_distortion_labels=cognitive_distortion_labels,
                attention_entropy=attention_entropy,
                contrastive_features=contrastive_features
            )
            results.update(loss_dict)

        return results

    def compute_loss(
        self,
        logits: torch.Tensor,
        labels: torch.Tensor,
        cdao_logits: torch.Tensor,
        cognitive_distortion_labels: Optional[torch.Tensor],
        attention_entropy: torch.Tensor,
        contrastive_features: torch.Tensor
    ) -> Dict[str, torch.Tensor]:
        """è®¡ç®—æ€»æŸå¤±"""

        # ä¸»ä»»åŠ¡æŸå¤±
        main_loss = F.cross_entropy(logits, labels)

        # æ³¨æ„åŠ›ç†µæ­£åˆ™åŒ–æŸå¤±
        entropy_loss = -attention_entropy

        # è®¤çŸ¥æ‰­æ›²æŸå¤±
        cdao_loss = torch.tensor(0.0, device=logits.device)
        if cognitive_distortion_labels is not None:
            cdao_loss = F.cross_entropy(cdao_logits, cognitive_distortion_labels)

        # å¯¹æ¯”å­¦ä¹ æŸå¤±
        contrastive_loss = self.compute_contrastive_loss(contrastive_features, labels)

        # æ€»æŸå¤±
        total_loss = (
            main_loss +
            self.config.entropy_weight * entropy_loss +
            self.config.cdao_weight * cdao_loss +
            self.config.contrastive_weight * contrastive_loss
        )

        return {
            'loss': total_loss,
            'main_loss': main_loss,
            'entropy_loss': entropy_loss,
            'cdao_loss': cdao_loss,
            'contrastive_loss': contrastive_loss
        }

    def compute_contrastive_loss(
        self,
        features: torch.Tensor,
        labels: torch.Tensor
    ) -> torch.Tensor:
        """è®¡ç®—ç›‘ç£å¯¹æ¯”å­¦ä¹ æŸå¤±"""
        device = features.device
        batch_size = features.shape[0]

        if batch_size <= 1:
            return torch.tensor(0.0, device=device)

        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        similarity_matrix = torch.matmul(features, features.T) / self.config.contrastive_temperature

        # åˆ›å»ºæ ‡ç­¾æ©ç 
        labels = labels.contiguous().view(-1, 1)
        mask = torch.eq(labels, labels.T).float().to(device)

        # ç§»é™¤å¯¹è§’çº¿
        diagonal_mask = torch.eye(batch_size, device=device)
        mask = mask * (1 - diagonal_mask)

        # è®¡ç®—å¯¹æ¯”æŸå¤±
        exp_sim = torch.exp(similarity_matrix)
        exp_sim = exp_sim * (1 - diagonal_mask)

        pos_sim = exp_sim * mask
        neg_sim = exp_sim * (1 - mask)

        pos_sum = pos_sim.sum(dim=1)
        neg_sum = neg_sim.sum(dim=1)

        pos_sum = torch.clamp(pos_sum, min=1e-8)
        total_sum = pos_sum + neg_sum

        loss = -torch.log(pos_sum / total_sum)

        mask_pos = (mask.sum(dim=1) > 0).float()
        loss = loss * mask_pos

        return loss.sum() / torch.clamp(mask_pos.sum(), min=1.0)


def test_enhanced_sraf():
    """æµ‹è¯•å¢å¼ºçš„SRAFæ¨¡å‹"""
    print("ğŸ§ª æµ‹è¯•å¢å¼ºçš„SRAFæ¨¡å‹...")

    # åˆ›å»ºé…ç½®
    class TestConfig:
        model_name = "bert-base-uncased"
        hidden_size = 768
        num_labels = 2
        dropout = 0.1
        num_cognitive_distortions = 5
        entropy_weight = 0.1
        cdao_weight = 0.05
        contrastive_weight = 0.1
        contrastive_temperature = 0.07

    config = TestConfig()

    # åˆ›å»ºæµ‹è¯•æ•°æ®
    batch_size, seq_len = 4, 128
    input_ids = torch.randint(0, 1000, (batch_size, seq_len))
    attention_mask = torch.ones(batch_size, seq_len)
    labels = torch.tensor([0, 1, 0, 1])

    # æµ‹è¯•ä¸åŒç±»å‹çš„ASRM
    asrm_types = ['span_aware', 'contrastive_friendly', 'hybrid_dcl']

    for asrm_type in asrm_types:
        print(f"\nğŸ”¬ æµ‹è¯• {asrm_type} ASRM...")

        try:
            model = EnhancedSRAFModel(config, asrm_type=asrm_type)
            outputs = model(input_ids, attention_mask, labels)

            print(f"âœ… {asrm_type} æµ‹è¯•é€šè¿‡:")
            print(f"  - åˆ†ç±»logits: {outputs['logits'].shape}")
            print(f"  - CDAO logits: {outputs['cdao_logits'].shape}")
            print(f"  - å¯¹æ¯”ç‰¹å¾: {outputs['contrastive_features'].shape}")
            print(f"  - æ€»æŸå¤±: {outputs['loss'].item():.4f}")

        except Exception as e:
            print(f"âŒ {asrm_type} æµ‹è¯•å¤±è´¥: {e}")

    print("\nğŸ‰ å¢å¼ºSRAFæ¨¡å‹æµ‹è¯•å®Œæˆï¼")


if __name__ == "__main__":
    test_enhanced_sraf()

