#!/usr/bin/env python3
"""
ASRMæ”¹è¿›å®éªŒ
å¯¹æ¯”åŸºçº¿BERTå’Œæ”¹è¿›ASRMçš„æ•ˆæœ
"""

import argparse
import json
import os
import random
from pathlib import Path

import numpy as np
import torch
import torch.nn as nn
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report
from torch.optim import AdamW
from tqdm import tqdm
from transformers import get_linear_schedule_with_warmup

# è®¾ç½®é•œåƒ
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'


class ASRMTrainer:
    """ASRMå®éªŒè®­ç»ƒå™¨"""

    def __init__(self, model, train_loader, dev_loader, test_loader,
                 learning_rate=2e-5, num_epochs=5, device='cpu',
                 model_name='asrm_model', output_dir='asrm_outputs'):

        self.model = model
        self.train_loader = train_loader
        self.dev_loader = dev_loader
        self.test_loader = test_loader
        self.num_epochs = num_epochs
        self.device = torch.device(device)
        self.model_name = model_name
        self.output_dir = Path(output_dir)

        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir.mkdir(exist_ok=True)

        # ç§»åŠ¨æ¨¡å‹åˆ°è®¾å¤‡
        self.model.to(self.device)

        # ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
        self.optimizer = AdamW(self.model.parameters(), lr=learning_rate, weight_decay=0.01)

        total_steps = len(train_loader) * num_epochs
        self.scheduler = get_linear_schedule_with_warmup(
            self.optimizer,
            num_warmup_steps=int(0.1 * total_steps),
            num_training_steps=total_steps
        )

        # æŸå¤±å‡½æ•°
        self.criterion = nn.CrossEntropyLoss()

        # è®­ç»ƒå†å²
        self.train_history = {
            'epochs': [],
            'train_losses': [],
            'train_accuracies': [],
            'val_accuracies': [],
            'val_f1_scores': []
        }

        # æœ€ä½³æ¨¡å‹
        self.best_val_f1 = 0.0
        self.best_model_path = self.output_dir / f'best_{model_name}.pt'

    def train_epoch(self, epoch):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()

        total_loss = 0
        all_predictions = []
        all_labels = []

        progress_bar = tqdm(self.train_loader, desc=f"[{self.model_name}] Epoch {epoch+1}/{self.num_epochs}")

        for batch in progress_bar:
            # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
            input_ids = batch['input_ids'].to(self.device)
            attention_mask = batch['attention_mask'].to(self.device)
            labels = batch['labels'].to(self.device)

            # å‰å‘ä¼ æ’­
            logits = self.model(input_ids, attention_mask)
            loss = self.criterion(logits, labels)

            # åå‘ä¼ æ’­
            self.optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            self.optimizer.step()
            self.scheduler.step()

            # ç»Ÿè®¡
            total_loss += loss.item()
            predictions = torch.argmax(logits, dim=-1)
            all_predictions.extend(predictions.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

            # æ›´æ–°è¿›åº¦æ¡
            progress_bar.set_postfix({'loss': f"{loss.item():.4f}"})

        # è®¡ç®—å¹³å‡æŸå¤±å’Œå‡†ç¡®ç‡
        avg_loss = total_loss / len(self.train_loader)
        accuracy = accuracy_score(all_labels, all_predictions)

        return {'avg_loss': avg_loss, 'accuracy': accuracy}

    def evaluate(self, data_loader, dataset_name="éªŒè¯"):
        """è¯„ä¼°æ¨¡å‹"""
        self.model.eval()

        all_predictions = []
        all_labels = []
        total_loss = 0

        with torch.no_grad():
            for batch in tqdm(data_loader, desc=f"[{self.model_name}] è¯„ä¼°{dataset_name}é›†"):
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                labels = batch['labels'].to(self.device)

                logits = self.model(input_ids, attention_mask)
                loss = self.criterion(logits, labels)
                total_loss += loss.item()

                predictions = torch.argmax(logits, dim=-1)
                all_predictions.extend(predictions.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())

        # è®¡ç®—æŒ‡æ ‡
        avg_loss = total_loss / len(data_loader)
        accuracy = accuracy_score(all_labels, all_predictions)
        precision, recall, f1, _ = precision_recall_fscore_support(
            all_labels, all_predictions, average='weighted', zero_division=0
        )

        # è¯¦ç»†åˆ†ç±»æŠ¥å‘Š
        if dataset_name == "æµ‹è¯•":
            print(f"\nğŸ“Š [{self.model_name}] è¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
            print(classification_report(all_labels, all_predictions,
                                      target_names=['æ­£å¸¸', 'ä»‡æ¨'], digits=4))

        return {
            'loss': avg_loss,
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1': f1
        }

    def train(self):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        print(f"\nğŸš€ å¼€å§‹è®­ç»ƒ {self.model_name} æ¨¡å‹...")

        for epoch in range(self.num_epochs):
            print(f"\n{'='*50}")
            print(f"[{self.model_name}] Epoch {epoch + 1}/{self.num_epochs}")
            print(f"{'='*50}")

            # è®­ç»ƒ
            train_metrics = self.train_epoch(epoch)

            # éªŒè¯
            val_metrics = self.evaluate(self.dev_loader, "éªŒè¯")

            # è®°å½•å†å²
            self.train_history['epochs'].append(epoch + 1)
            self.train_history['train_losses'].append(train_metrics['avg_loss'])
            self.train_history['train_accuracies'].append(train_metrics['accuracy'])
            self.train_history['val_accuracies'].append(val_metrics['accuracy'])
            self.train_history['val_f1_scores'].append(val_metrics['f1'])

            # æ‰“å°æŒ‡æ ‡
            print(f"\nğŸ“Š [{self.model_name}] è®­ç»ƒæŒ‡æ ‡:")
            print(f"  - æŸå¤±: {train_metrics['avg_loss']:.4f}")
            print(f"  - å‡†ç¡®ç‡: {train_metrics['accuracy']:.4f}")

            print(f"\nğŸ“ˆ [{self.model_name}] éªŒè¯æŒ‡æ ‡:")
            print(f"  - å‡†ç¡®ç‡: {val_metrics['accuracy']:.4f}")
            print(f"  - F1åˆ†æ•°: {val_metrics['f1']:.4f}")
            print(f"  - ç²¾ç¡®ç‡: {val_metrics['precision']:.4f}")
            print(f"  - å¬å›ç‡: {val_metrics['recall']:.4f}")

            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_metrics['f1'] > self.best_val_f1:
                self.best_val_f1 = val_metrics['f1']
                self.save_model()
                print(f"ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ŒéªŒè¯F1: {val_metrics['f1']:.4f}")

        # æµ‹è¯•æœ€ä½³æ¨¡å‹
        print(f"\nğŸ§ª [{self.model_name}] åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æœ€ä½³æ¨¡å‹...")
        self.load_model()
        test_metrics = self.evaluate(self.test_loader, "æµ‹è¯•")

        print(f"\nğŸ¯ [{self.model_name}] æœ€ç»ˆæµ‹è¯•ç»“æœ:")
        print(f"  - å‡†ç¡®ç‡: {test_metrics['accuracy']:.4f}")
        print(f"  - F1åˆ†æ•°: {test_metrics['f1']:.4f}")
        print(f"  - ç²¾ç¡®ç‡: {test_metrics['precision']:.4f}")
        print(f"  - å¬å›ç‡: {test_metrics['recall']:.4f}")

        # ä¿å­˜ç»“æœ
        results = {
            'model_name': self.model_name,
            'test_accuracy': test_metrics['accuracy'],
            'test_f1': test_metrics['f1'],
            'test_precision': test_metrics['precision'],
            'test_recall': test_metrics['recall'],
            'best_val_f1': self.best_val_f1,
            'train_history': self.train_history
        }

        results_path = self.output_dir / f'{self.model_name}_results.json'
        with open(results_path, 'w') as f:
            json.dump(results, f, indent=2, default=str)

        return test_metrics

    def save_model(self):
        """ä¿å­˜æ¨¡å‹"""
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'best_val_f1': self.best_val_f1,
            'train_history': self.train_history
        }, self.best_model_path)

    def load_model(self):
        """åŠ è½½æ¨¡å‹"""
        checkpoint = torch.load(self.best_model_path, map_location=self.device)
        self.model.load_state_dict(checkpoint['model_state_dict'])


def run_asrm_experiments(args):
    """è¿è¡ŒASRMæ”¹è¿›å®éªŒ"""

    print("="*80)
    print("ğŸ”¬ ASRMæ”¹è¿›å®éªŒ")
    print("="*80)
    print(f"ğŸ“Š å®éªŒé…ç½®:")
    print(f"  - æ‰¹æ¬¡å¤§å°: {args.batch_size}")
    print(f"  - å­¦ä¹ ç‡: {args.learning_rate}")
    print(f"  - è®­ç»ƒè½®æ•°: {args.num_epochs}")
    print(f"  - æœ€å¤§é•¿åº¦: {args.max_length}")
    print(f"  - è®¾å¤‡: {args.device}")
    print(f"  - ASRMç±»å‹: {args.asrm_type}")
    print("="*80)

    # åŠ è½½æ•°æ®
    print("\nğŸ“Š åŠ è½½æ•°æ®...")
    from simple_data_loader import load_simple_data
    train_loader, dev_loader, test_loader, tokenizer = load_simple_data(
        batch_size=args.batch_size,
        max_length=args.max_length
    )

    # å®éªŒç»“æœå­˜å‚¨
    all_results = {}

    # 1. åŸºçº¿å®éªŒ
    print("\n" + "="*60)
    print("ğŸ å®éªŒ1: BERTåŸºçº¿")
    print("="*60)

    from baseline_model import BERTBaseline
    baseline_model = BERTBaseline()

    baseline_trainer = ASRMTrainer(
        model=baseline_model,
        train_loader=train_loader,
        dev_loader=dev_loader,
        test_loader=test_loader,
        learning_rate=args.learning_rate,
        num_epochs=args.num_epochs,
        device=args.device,
        model_name='bert_baseline',
        output_dir=args.output_dir
    )

    baseline_results = baseline_trainer.train()
    all_results['baseline'] = baseline_results

    # 2. ASRMå®éªŒ
    print("\n" + "="*60)
    print(f"ğŸš€ å®éªŒ2: BERT + {args.asrm_type.upper()} ASRM")
    print("="*60)

    from baseline_model import BERTWithASRM
    asrm_model = BERTWithASRM(asrm_type=args.asrm_type)

    asrm_trainer = ASRMTrainer(
        model=asrm_model,
        train_loader=train_loader,
        dev_loader=dev_loader,
        test_loader=test_loader,
        learning_rate=args.learning_rate,
        num_epochs=args.num_epochs,
        device=args.device,
        model_name=f'bert_{args.asrm_type}_asrm',
        output_dir=args.output_dir
    )

    asrm_results = asrm_trainer.train()
    all_results['asrm'] = asrm_results

    # 3. ç»“æœå¯¹æ¯”
    print("\n" + "="*80)
    print("ğŸ“ˆ å®éªŒç»“æœå¯¹æ¯”")
    print("="*80)

    print(f"ğŸ BERTåŸºçº¿:")
    print(f"  - å‡†ç¡®ç‡: {baseline_results['accuracy']:.4f}")
    print(f"  - F1åˆ†æ•°: {baseline_results['f1']:.4f}")
    print(f"  - ç²¾ç¡®ç‡: {baseline_results['precision']:.4f}")
    print(f"  - å¬å›ç‡: {baseline_results['recall']:.4f}")

    print(f"\nğŸš€ BERT + {args.asrm_type.upper()} ASRM:")
    print(f"  - å‡†ç¡®ç‡: {asrm_results['accuracy']:.4f}")
    print(f"  - F1åˆ†æ•°: {asrm_results['f1']:.4f}")
    print(f"  - ç²¾ç¡®ç‡: {asrm_results['precision']:.4f}")
    print(f"  - å¬å›ç‡: {asrm_results['recall']:.4f}")

    # è®¡ç®—æ”¹è¿›å¹…åº¦
    accuracy_improvement = (asrm_results['accuracy'] - baseline_results['accuracy']) / baseline_results['accuracy'] * 100
    f1_improvement = (asrm_results['f1'] - baseline_results['f1']) / baseline_results['f1'] * 100

    print(f"\nğŸ“Š æ”¹è¿›å¹…åº¦:")
    print(f"  - å‡†ç¡®ç‡æ”¹è¿›: {accuracy_improvement:+.2f}%")
    print(f"  - F1åˆ†æ•°æ”¹è¿›: {f1_improvement:+.2f}%")

    if f1_improvement > 5:
        print("ğŸ‰ æ˜¾è‘—æ”¹è¿›ï¼ASRMæ¨¡å—æœ‰æ•ˆï¼")
    elif f1_improvement > 0:
        print("âœ… æœ‰æ‰€æ”¹è¿›ï¼ŒASRMæ¨¡å—æœ‰ä¸€å®šæ•ˆæœ")
    else:
        print("âŒ æ€§èƒ½ä¸‹é™ï¼Œéœ€è¦è¿›ä¸€æ­¥è°ƒä¼˜")

    # ä¿å­˜å¯¹æ¯”ç»“æœ
    comparison_results = {
        'baseline': baseline_results,
        'asrm': asrm_results,
        'improvements': {
            'accuracy': accuracy_improvement,
            'f1': f1_improvement
        },
        'config': vars(args)
    }

    comparison_path = Path(args.output_dir) / 'asrm_comparison_results.json'
    with open(comparison_path, 'w') as f:
        json.dump(comparison_results, f, indent=2, default=str)

    return comparison_results


def main():
    parser = argparse.ArgumentParser(description="ASRMæ”¹è¿›å®éªŒ")
    parser.add_argument("--batch_size", type=int, default=16, help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--learning_rate", type=float, default=2e-5, help="å­¦ä¹ ç‡")
    parser.add_argument("--num_epochs", type=int, default=5, help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--max_length", type=int, default=128, help="æœ€å¤§åºåˆ—é•¿åº¦")
    parser.add_argument("--device", type=str, default="cpu", help="è®¾å¤‡")
    parser.add_argument("--output_dir", type=str, default="asrm_outputs", help="è¾“å‡ºç›®å½•")
    parser.add_argument("--asrm_type", type=str, default="improved",
                       choices=['improved', 'adaptive', 'multiscale'], help="ASRMç±»å‹")
    parser.add_argument("--quick_test", action="store_true", help="å¿«é€Ÿæµ‹è¯•æ¨¡å¼")

    args = parser.parse_args()

    # å¿«é€Ÿæµ‹è¯•æ¨¡å¼
    if args.quick_test:
        args.batch_size = 8
        args.num_epochs = 3
        args.max_length = 64
        args.output_dir = "asrm_test_outputs"
        print("ğŸš€ å¿«é€Ÿæµ‹è¯•æ¨¡å¼")

    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(42)
    np.random.seed(42)
    random.seed(42)

    # è¿è¡Œå®éªŒ
    results = run_asrm_experiments(args)


if __name__ == "__main__":
    main()

