#!/usr/bin/env python3
"""
Span-aware ASRMæ¨¡å—
åŸºäºæ·±åº¦åˆ†æï¼Œé‡æ–°è®¾è®¡ASRMä»¥æ•è·span-levelè¯­ä¹‰ä¿¡æ¯
è§£å†³åŸå§‹ASRMçš„æ ¹æœ¬é—®é¢˜
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Optional, List


class SpanAwareASRM(nn.Module):
    """
    Spanæ„ŸçŸ¥çš„ASRMæ¨¡å—

    æ ¸å¿ƒæ”¹è¿›ï¼š
    1. ä»token-levelåˆ°span-levelå»ºæ¨¡
    2. ä¿æŒä½ç½®å’Œç»“æ„ä¿¡æ¯
    3. å¢å¼ºç‰¹å¾åˆ¤åˆ«æ€§
    4. é€‚é…å¯¹æ¯”å­¦ä¹ éœ€æ±‚
    """

    def __init__(self, hidden_size: int = 768, window_sizes: List[int] = [1, 3, 5],
                 reduction_ratio: int = 4):
        super(SpanAwareASRM, self).__init__()

        self.hidden_size = hidden_size
        self.window_sizes = window_sizes
        self.num_windows = len(window_sizes)

        # å¤šçª—å£å·ç§¯å±‚ - æ•è·ä¸åŒé•¿åº¦çš„span
        self.span_convs = nn.ModuleList([
            nn.Conv1d(hidden_size, hidden_size, kernel_size=ws,
                     padding=ws//2, groups=hidden_size//4)  # ä½¿ç”¨åˆ†ç»„å·ç§¯å‡å°‘å‚æ•°
            for ws in window_sizes
        ])

        # Spanèåˆå±‚
        self.span_fusion = nn.Sequential(
            nn.Linear(hidden_size * self.num_windows, hidden_size),
            nn.LayerNorm(hidden_size),
            nn.GELU()
        )

        # ä½ç½®æ„ŸçŸ¥çš„æ³¨æ„åŠ›æœºåˆ¶
        self.position_attention = nn.MultiheadAttention(
            embed_dim=hidden_size,
            num_heads=8,
            dropout=0.1,
            batch_first=True
        )

        # è¯­ä¹‰é‡è¦æ€§é¢„æµ‹å™¨
        self.importance_predictor = nn.Sequential(
            nn.Linear(hidden_size, hidden_size // reduction_ratio),
            nn.GELU(),
            nn.Linear(hidden_size // reduction_ratio, 1),
            nn.Sigmoid()
        )

        # å¯¹æ¯”å­¦ä¹ å‹å¥½çš„ç‰¹å¾å¢å¼ºå™¨
        self.contrastive_enhancer = nn.Sequential(
            nn.Linear(hidden_size, hidden_size),
            nn.LayerNorm(hidden_size),
            nn.GELU(),
            nn.Linear(hidden_size, hidden_size)
        )

        # æ®‹å·®è¿æ¥æƒé‡
        self.residual_weight = nn.Parameter(torch.tensor(0.5))

    def forward(self, hidden_states: torch.Tensor,
                attention_mask: Optional[torch.Tensor] = None,
                labels: Optional[torch.Tensor] = None,
                apply_augmentation: bool = True) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­

        Args:
            hidden_states: [batch_size, seq_len, hidden_size]
            attention_mask: [batch_size, seq_len]
            labels: [batch_size] - ç”¨äºå¯¹æ¯”å­¦ä¹ å¢å¼º
            apply_augmentation: æ˜¯å¦åº”ç”¨å¢å¼º
        """
        batch_size, seq_len, hidden_size = hidden_states.shape

        # 1. Span-levelç‰¹å¾æå–
        span_features = self._extract_span_features(hidden_states)

        # 2. ä½ç½®æ„ŸçŸ¥çš„æ³¨æ„åŠ›
        position_enhanced_features = self._apply_position_attention(
            span_features, attention_mask
        )

        # 3. è¯­ä¹‰é‡è¦æ€§è®¡ç®—
        importance_scores = self.importance_predictor(position_enhanced_features)

        # 4. ç‰¹å¾æ ¡å‡†
        calibrated_features = position_enhanced_features * importance_scores

        # 5. å¯¹æ¯”å­¦ä¹ å‹å¥½çš„å¢å¼º
        if apply_augmentation and self.training:
            enhanced_features = self._apply_contrastive_enhancement(
                calibrated_features, labels
            )
        else:
            enhanced_features = calibrated_features

        # 6. æ®‹å·®è¿æ¥
        output = (
            torch.sigmoid(self.residual_weight) * enhanced_features +
            (1 - torch.sigmoid(self.residual_weight)) * hidden_states
        )

        return output

    def _extract_span_features(self, hidden_states: torch.Tensor) -> torch.Tensor:
        """æå–å¤šå°ºåº¦spanç‰¹å¾"""
        batch_size, seq_len, hidden_size = hidden_states.shape

        # è½¬æ¢ä¸ºå·ç§¯æ ¼å¼ [batch_size, hidden_size, seq_len]
        conv_input = hidden_states.transpose(1, 2)

        span_outputs = []
        for conv in self.span_convs:
            # åº”ç”¨å·ç§¯è·å–spanç‰¹å¾
            span_output = conv(conv_input)  # [batch_size, hidden_size, seq_len]
            span_output = F.gelu(span_output)
            span_outputs.append(span_output.transpose(1, 2))  # è½¬å› [batch_size, seq_len, hidden_size]

        # æ‹¼æ¥å¤šå°ºåº¦ç‰¹å¾
        concatenated = torch.cat(span_outputs, dim=-1)  # [batch_size, seq_len, hidden_size * num_windows]

        # èåˆå¤šå°ºåº¦ç‰¹å¾
        fused_features = self.span_fusion(concatenated)  # [batch_size, seq_len, hidden_size]

        return fused_features

    def _apply_position_attention(self, features: torch.Tensor,
                                attention_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        """åº”ç”¨ä½ç½®æ„ŸçŸ¥çš„æ³¨æ„åŠ›æœºåˆ¶"""

        # åˆ›å»ºæ³¨æ„åŠ›æ©ç 
        if attention_mask is not None:
            # è½¬æ¢ä¸ºæ³¨æ„åŠ›æ©ç æ ¼å¼
            attn_mask = attention_mask.unsqueeze(1).expand(-1, features.size(1), -1)
            attn_mask = attn_mask.masked_fill(attn_mask == 0, float('-inf'))
            attn_mask = attn_mask.masked_fill(attn_mask == 1, 0.0)
        else:
            attn_mask = None

        # è‡ªæ³¨æ„åŠ›æœºåˆ¶
        attended_features, _ = self.position_attention(
            features, features, features,
            attn_mask=attn_mask
        )

        return attended_features

    def _apply_contrastive_enhancement(self, features: torch.Tensor,
                                     labels: Optional[torch.Tensor] = None) -> torch.Tensor:
        """åº”ç”¨å¯¹æ¯”å­¦ä¹ å‹å¥½çš„ç‰¹å¾å¢å¼º"""

        # åŸºç¡€ç‰¹å¾å¢å¼º
        enhanced = self.contrastive_enhancer(features)

        if labels is not None and self.training:
            # ç±»åˆ«æ„ŸçŸ¥çš„ç‰¹å¾å¢å¼º
            enhanced = self._enhance_inter_class_difference(enhanced, labels)

        return enhanced

    def _enhance_inter_class_difference(self, features: torch.Tensor,
                                      labels: torch.Tensor) -> torch.Tensor:
        """å¢å¼ºç±»é—´å·®å¼‚"""
        batch_size = features.size(0)

        # è®¡ç®—ç±»åˆ«ä¸­å¿ƒ
        unique_labels = torch.unique(labels)
        class_centers = []

        for label in unique_labels:
            mask = (labels == label)
            if mask.sum() > 0:
                class_center = features[mask].mean(dim=0, keepdim=True)
                class_centers.append(class_center)

        if len(class_centers) > 1:
            # è®¡ç®—ç±»é—´è·ç¦»
            center_diff = class_centers[0] - class_centers[1]

            # å¢å¼ºç±»é—´å·®å¼‚
            for i, label in enumerate(labels):
                if label == unique_labels[0]:
                    features[i] = features[i] + 0.1 * center_diff.squeeze(0)
                else:
                    features[i] = features[i] - 0.1 * center_diff.squeeze(0)

        return features


class ContrastiveFriendlyASRM(nn.Module):
    """
    å¯¹æ¯”å­¦ä¹ å‹å¥½çš„ASRMæ¨¡å—
    ä¸“é—¨ä¸ºå¯¹æ¯”å­¦ä¹ ä»»åŠ¡è®¾è®¡
    """

    def __init__(self, hidden_size: int = 768, temperature: float = 0.07):
        super(ContrastiveFriendlyASRM, self).__init__()

        self.hidden_size = hidden_size
        self.temperature = temperature

        # Span-awareåŸºç¡€æ¨¡å—
        self.span_asrm = SpanAwareASRM(hidden_size)

        # å¯¹æ¯”å­¦ä¹ ç‰¹å®šçš„æŠ•å½±å¤´
        self.projection_head = nn.Sequential(
            nn.Linear(hidden_size, hidden_size),
            nn.GELU(),
            nn.Linear(hidden_size, hidden_size // 2)
        )

        # ç‰¹å¾åˆ¤åˆ«å™¨
        self.discriminator = nn.Sequential(
            nn.Linear(hidden_size, hidden_size // 4),
            nn.GELU(),
            nn.Linear(hidden_size // 4, 2)  # äºŒåˆ†ç±»
        )

    def forward(self, hidden_states: torch.Tensor,
                attention_mask: Optional[torch.Tensor] = None,
                labels: Optional[torch.Tensor] = None,
                return_contrastive_features: bool = False) -> dict:
        """
        å‰å‘ä¼ æ’­ï¼Œè¿”å›å¤šç§ç‰¹å¾ç”¨äºå¯¹æ¯”å­¦ä¹ 
        """

        # åŸºç¡€span-awareå¢å¼º
        enhanced_features = self.span_asrm(
            hidden_states, attention_mask, labels, apply_augmentation=True
        )

        # CLSè¡¨ç¤º
        cls_features = enhanced_features[:, 0, :]  # [batch_size, hidden_size]

        # å¯¹æ¯”å­¦ä¹ æŠ•å½±
        contrastive_features = self.projection_head(cls_features)
        contrastive_features = F.normalize(contrastive_features, p=2, dim=1)

        # åˆ†ç±»ç‰¹å¾
        classification_logits = self.discriminator(cls_features)

        results = {
            'enhanced_features': enhanced_features,
            'cls_features': cls_features,
            'classification_logits': classification_logits
        }

        if return_contrastive_features:
            results['contrastive_features'] = contrastive_features

        return results


class HybridDCLASRM(nn.Module):
    """
    æ··åˆDCL-ASRMæ¨¡å—
    ç»“åˆåŒé‡å¯¹æ¯”å­¦ä¹ å’ŒSpan-aware ASRMçš„ä¼˜åŠ¿
    """

    def __init__(self, hidden_size: int = 768, num_classes: int = 2):
        super(HybridDCLASRM, self).__init__()

        self.hidden_size = hidden_size
        self.num_classes = num_classes

        # Span-aware ASRMæ ¸å¿ƒ
        self.span_asrm = SpanAwareASRM(hidden_size)

        # è‡ªç›‘ç£å¯¹æ¯”å­¦ä¹ åˆ†æ”¯
        self.self_supervised_head = nn.Sequential(
            nn.Linear(hidden_size, hidden_size),
            nn.GELU(),
            nn.Linear(hidden_size, 128)
        )

        # ç›‘ç£å¯¹æ¯”å­¦ä¹ åˆ†æ”¯
        self.supervised_head = nn.Sequential(
            nn.Linear(hidden_size, hidden_size),
            nn.GELU(),
            nn.Linear(hidden_size, 128)
        )

        # ä¸»åˆ†ç±»å™¨
        self.classifier = nn.Sequential(
            nn.Linear(hidden_size, hidden_size // 2),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_size // 2, num_classes)
        )

        # ç„¦ç‚¹æŸå¤±æƒé‡
        self.focal_alpha = nn.Parameter(torch.tensor(1.0))
        self.focal_gamma = nn.Parameter(torch.tensor(2.0))

    def forward(self, hidden_states: torch.Tensor,
                attention_mask: Optional[torch.Tensor] = None,
                labels: Optional[torch.Tensor] = None) -> dict:
        """
        å‰å‘ä¼ æ’­ï¼Œå®ç°æ··åˆDCL-ASRM
        """

        # Span-awareç‰¹å¾å¢å¼º
        enhanced_features = self.span_asrm(
            hidden_states, attention_mask, labels, apply_augmentation=True
        )

        # CLSè¡¨ç¤º
        cls_features = enhanced_features[:, 0, :]

        # åˆ†ç±»è¾“å‡º
        classification_logits = self.classifier(cls_features)

        # å¯¹æ¯”å­¦ä¹ ç‰¹å¾
        self_supervised_features = F.normalize(
            self.self_supervised_head(cls_features), p=2, dim=1
        )
        supervised_features = F.normalize(
            self.supervised_head(cls_features), p=2, dim=1
        )

        return {
            'classification_logits': classification_logits,
            'self_supervised_features': self_supervised_features,
            'supervised_features': supervised_features,
            'enhanced_features': enhanced_features,
            'cls_features': cls_features
        }


def test_span_aware_asrm():
    """æµ‹è¯•Span-aware ASRMæ¨¡å—"""
    print("ğŸ§ª æµ‹è¯•Span-aware ASRMæ¨¡å—...")

    # åˆ›å»ºæµ‹è¯•æ•°æ®
    batch_size, seq_len, hidden_size = 4, 128, 768
    hidden_states = torch.randn(batch_size, seq_len, hidden_size)
    attention_mask = torch.ones(batch_size, seq_len)
    labels = torch.tensor([0, 1, 0, 1])

    # æµ‹è¯•åŸºç¡€Span-aware ASRM
    print("\n1ï¸âƒ£ æµ‹è¯•åŸºç¡€Span-aware ASRM...")
    span_asrm = SpanAwareASRM(hidden_size=hidden_size)
    output1 = span_asrm(hidden_states, attention_mask, labels)
    print(f"âœ… è¾“å…¥: {hidden_states.shape} -> è¾“å‡º: {output1.shape}")

    # æµ‹è¯•å¯¹æ¯”å­¦ä¹ å‹å¥½çš„ASRM
    print("\n2ï¸âƒ£ æµ‹è¯•å¯¹æ¯”å­¦ä¹ å‹å¥½çš„ASRM...")
    contrastive_asrm = ContrastiveFriendlyASRM(hidden_size=hidden_size)
    output2 = contrastive_asrm(hidden_states, attention_mask, labels, return_contrastive_features=True)
    print(f"âœ… åˆ†ç±»logits: {output2['classification_logits'].shape}")
    print(f"âœ… å¯¹æ¯”ç‰¹å¾: {output2['contrastive_features'].shape}")

    # æµ‹è¯•æ··åˆDCL-ASRM
    print("\n3ï¸âƒ£ æµ‹è¯•æ··åˆDCL-ASRM...")
    hybrid_asrm = HybridDCLASRM(hidden_size=hidden_size)
    output3 = hybrid_asrm(hidden_states, attention_mask, labels)
    print(f"âœ… åˆ†ç±»logits: {output3['classification_logits'].shape}")
    print(f"âœ… è‡ªç›‘ç£ç‰¹å¾: {output3['self_supervised_features'].shape}")
    print(f"âœ… ç›‘ç£ç‰¹å¾: {output3['supervised_features'].shape}")

    print("\nğŸ‰ æ‰€æœ‰Span-aware ASRMæ¨¡å—æµ‹è¯•é€šè¿‡ï¼")


if __name__ == "__main__":
    test_span_aware_asrm()

