#!/usr/bin/env python3
"""
ç®€å•çš„BERTåŸºçº¿æ¨¡å‹
ç”¨äºä¸æ”¹è¿›çš„ASRMæ¨¡å‹è¿›è¡Œå¯¹æ¯”
"""

import torch
import torch.nn as nn
from transformers import AutoModel


class BERTBaseline(nn.Module):
    """ç®€å•çš„BERTåˆ†ç±»å™¨åŸºçº¿"""

    def __init__(self, model_name: str = 'bert-base-uncased', num_classes: int = 2,
                 dropout_rate: float = 0.1):
        super(BERTBaseline, self).__init__()

        # BERTç¼–ç å™¨
        self.bert = AutoModel.from_pretrained(model_name)
        self.hidden_size = self.bert.config.hidden_size

        # åˆ†ç±»å¤´
        self.dropout = nn.Dropout(dropout_rate)
        self.classifier = nn.Linear(self.hidden_size, num_classes)

    def forward(self, input_ids, attention_mask):
        """å‰å‘ä¼ æ’­"""
        # BERTç¼–ç 
        outputs = self.bert(
            input_ids=input_ids,
            attention_mask=attention_mask
        )

        # ä½¿ç”¨[CLS]tokençš„è¡¨ç¤º
        pooled_output = outputs.pooler_output

        # Dropoutå’Œåˆ†ç±»
        pooled_output = self.dropout(pooled_output)
        logits = self.classifier(pooled_output)

        return logits


class BERTWithASRM(nn.Module):
    """é›†æˆæ”¹è¿›ASRMçš„BERTæ¨¡å‹"""

    def __init__(self, model_name: str = 'bert-base-uncased', num_classes: int = 2,
                 dropout_rate: float = 0.1, asrm_type: str = 'improved'):
        super(BERTWithASRM, self).__init__()

        # BERTç¼–ç å™¨
        self.bert = AutoModel.from_pretrained(model_name)
        self.hidden_size = self.bert.config.hidden_size

        # å¯¼å…¥æ”¹è¿›çš„ASRMæ¨¡å—
        from improved_asrm import ImprovedASRM, AdaptiveASRM, MultiScaleASRM

        # é€‰æ‹©ASRMç±»å‹
        if asrm_type == 'improved':
            self.asrm = ImprovedASRM(self.hidden_size)
        elif asrm_type == 'adaptive':
            self.asrm = AdaptiveASRM(self.hidden_size)
        elif asrm_type == 'multiscale':
            self.asrm = MultiScaleASRM(self.hidden_size)
        else:
            raise ValueError(f"Unknown ASRM type: {asrm_type}")

        # åˆ†ç±»å¤´
        self.dropout = nn.Dropout(dropout_rate)
        self.classifier = nn.Linear(self.hidden_size, num_classes)

    def forward(self, input_ids, attention_mask):
        """å‰å‘ä¼ æ’­"""
        # BERTç¼–ç 
        outputs = self.bert(
            input_ids=input_ids,
            attention_mask=attention_mask,
            output_hidden_states=True
        )

        # è·å–æœ€åä¸€å±‚éšè—çŠ¶æ€
        hidden_states = outputs.last_hidden_state  # [batch_size, seq_len, hidden_size]

        # åº”ç”¨æ”¹è¿›çš„ASRM
        enhanced_states = self.asrm(hidden_states, apply_augmentation=True)

        # è·å–[CLS]è¡¨ç¤º
        cls_representation = enhanced_states[:, 0, :]  # [batch_size, hidden_size]

        # åˆ†ç±»
        pooled_output = self.dropout(cls_representation)
        logits = self.classifier(pooled_output)

        return logits


def test_models():
    """æµ‹è¯•æ¨¡å‹"""
    print("ğŸ§ª æµ‹è¯•åŸºçº¿æ¨¡å‹...")

    # åˆ›å»ºæµ‹è¯•æ•°æ®
    batch_size, seq_len = 2, 128
    input_ids = torch.randint(0, 1000, (batch_size, seq_len))
    attention_mask = torch.ones(batch_size, seq_len)

    # æµ‹è¯•åŸºçº¿æ¨¡å‹
    print("\n1ï¸âƒ£ æµ‹è¯•BERTåŸºçº¿...")
    baseline_model = BERTBaseline()
    baseline_output = baseline_model(input_ids, attention_mask)
    print(f"âœ… åŸºçº¿è¾“å‡ºå½¢çŠ¶: {baseline_output.shape}")

    # æµ‹è¯•ASRMæ¨¡å‹
    print("\n2ï¸âƒ£ æµ‹è¯•BERT+æ”¹è¿›ASRM...")
    asrm_model = BERTWithASRM(asrm_type='improved')
    asrm_output = asrm_model(input_ids, attention_mask)
    print(f"âœ… ASRMè¾“å‡ºå½¢çŠ¶: {asrm_output.shape}")

    print("\nğŸ‰ æ¨¡å‹æµ‹è¯•é€šè¿‡ï¼")


if __name__ == "__main__":
    test_models()

