#!/usr/bin/env python3
"""
BERT Baseline ä¸»ç¨‹åº
æœ€ç®€å•çš„ä»‡æ¨è¨€è®ºæ£€æµ‹å®éªŒ
"""

import torch
import argparse
from baseline_model import BERTBaseline
from baseline_trainer import BaselineTrainer
from simple_data_loader import load_simple_data

def main():
    # å‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description="BERT Baseline ä»‡æ¨è¨€è®ºæ£€æµ‹")
    parser.add_argument("--batch_size", type=int, default=16, help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--learning_rate", type=float, default=2e-5, help="å­¦ä¹ ç‡")
    parser.add_argument("--num_epochs", type=int, default=3, help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--max_length", type=int, default=128, help="æœ€å¤§åºåˆ—é•¿åº¦")
    parser.add_argument("--device", type=str, default="cpu", help="è®¾å¤‡")

    args = parser.parse_args()

    print("="*50)
    print("BERT Baseline ä»‡æ¨è¨€è®ºæ£€æµ‹")
    print("="*50)
    print(f"è®¾å¤‡: {args.device}")
    print(f"æ‰¹æ¬¡å¤§å°: {args.batch_size}")
    print(f"å­¦ä¹ ç‡: {args.learning_rate}")
    print(f"è®­ç»ƒè½®æ•°: {args.num_epochs}")
    print(f"æœ€å¤§é•¿åº¦: {args.max_length}")
    print("="*50)

    try:
        # åŠ è½½æ•°æ®
        train_loader, dev_loader, test_loader, tokenizer = load_simple_data(
            batch_size=args.batch_size,
            max_length=args.max_length
        )

        # åˆ›å»ºæ¨¡å‹
        print("\nåˆ›å»ºBERTæ¨¡å‹...")
        model = BERTBaseline()
        param_count = sum(p.numel() for p in model.parameters())
        print(f"æ¨¡å‹å‚æ•°é‡: {param_count:,}")

        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = BaselineTrainer(
            model=model,
            train_loader=train_loader,
            dev_loader=dev_loader,
            test_loader=test_loader,
            learning_rate=args.learning_rate,
            num_epochs=args.num_epochs,
            device=args.device
        )

        # è®­ç»ƒ
        test_metrics = trainer.train()

        print(f"\nğŸ‰ è®­ç»ƒå®Œæˆï¼")
        print(f"ğŸ“Š æœ€ç»ˆæµ‹è¯•ç»“æœ:")
        print(f"  - å‡†ç¡®ç‡: {test_metrics['accuracy']:.4f}")
        print(f"  - ç²¾ç¡®ç‡: {test_metrics['precision']:.4f}")
        print(f"  - å¬å›ç‡: {test_metrics['recall']:.4f}")
        print(f"  - F1åˆ†æ•°: {test_metrics['f1']:.4f}")

        # é¢„æµ‹ç¤ºä¾‹
        print(f"\nğŸ”® é¢„æµ‹ç¤ºä¾‹:")
        sample_texts = [
            "I love everyone regardless of their background.",
            "These people are ruining our country.",
            "Diversity makes us stronger.",
            "We need to get rid of these immigrants."
        ]

        predictions = trainer.predict(sample_texts, tokenizer, args.max_length)

        for i, pred in enumerate(predictions, 1):
            print(f"\nç¤ºä¾‹ {i}:")
            print(f"  æ–‡æœ¬: {pred['text']}")
            print(f"  é¢„æµ‹: {pred['predicted_class']}")
            print(f"  ç½®ä¿¡åº¦: {pred['confidence']:.4f}")

    except Exception as e:
        print(f"âŒ è¿è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()

